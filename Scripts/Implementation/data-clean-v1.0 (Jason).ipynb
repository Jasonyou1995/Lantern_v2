{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords Corpus\n",
      "\n",
      "This corpus contains lists of stop words for several languages.  These\n",
      "are high-frequency grammatical words which are usually ignored in text\n",
      "retrieval applications.\n",
      "\n",
      "They were obtained from:\n",
      "http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/\n",
      "\n",
      "The English list has been augmented\n",
      "https://github.com/nltk/nltk_data/issues/22\n",
      "\n",
      "The German list has been corrected\n",
      "https://github.com/nltk/nltk_data/pull/49\n",
      "\n",
      "A Kazakh list has been added\n",
      "https://github.com/nltk/nltk_data/pull/52\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages and read from ./data repo\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "corpus_root = 'data'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
    "ids = wordlists.fileids()\n",
    "\n",
    "# use three documents as samples\n",
    "d1 = wordlists.words(ids[0])\n",
    "d2 = wordlists.words(ids[1])\n",
    "d3 = wordlists.words(ids[2])\n",
    "\n",
    "print(stopwords.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727 1001 976\n"
     ]
    }
   ],
   "source": [
    "# Get the frequency distribution for each document\n",
    "fd1 = nltk.FreqDist(d1)\n",
    "fd2 = nltk.FreqDist(d2)\n",
    "fd3 = nltk.FreqDist(d3)\n",
    "\n",
    "# the words that only shown once\n",
    "print(\n",
    "    len(fd1.hapaxes()),\n",
    "    len(fd2.hapaxes()),\n",
    "    len(fd3.hapaxes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 497), ('the', 229), (',', 226), ('of', 186), ('-', 161), ('to', 122), ('in', 118), ('and', 113), ('students', 99), ('use', 85)]\n",
      "[('.', 840), (',', 461), ('(', 303), (')', 233), ('and', 218), ('of', 202), ('1', 187), ('the', 175), ('4', 136), ('-', 133)]\n",
      "[('.', 677), (',', 605), ('1', 298), (')', 266), ('of', 215), ('testing', 207), ('and', 185), ('(.', 178), ('-', 149), ('use', 146)]\n"
     ]
    }
   ],
   "source": [
    "# most common words before cleaning\n",
    "print(fd1.most_common(10))\n",
    "print(fd2.most_common(10))\n",
    "print(fd3.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_stopwords = set(['the', 'a', '.'])  # we can define our own stop words in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosen 52 words from document 1\n",
      "Choosen 55 words from document 2\n",
      "Choosen 57 words from document 3\n"
     ]
    }
   ],
   "source": [
    "# Returns a set of word-frequency pairs chosen based on given limitations\n",
    "# param@words: list of words from input documents\n",
    "# param@freq: frequency distributions for each word from words\n",
    "# param@min_len: the minimum length of a word to be chosen\n",
    "# param@min_freq: the minimum times a word appear in the doc\n",
    "def feature_extractor(words, freq, min_len, min_freq, my_stop=set()):\n",
    "    return(set([(w.lower(), freq.get(w))\n",
    "           for w in set(words)\n",
    "           if len(w) >= min_len\n",
    "           and freq.get(w) >= min_freq\n",
    "           and w not in stopwords.words(fileids='english')\n",
    "           and w not in my_stopwords]))\n",
    "    # I want to add a filter to check whether the 'w' is a English word (with Lexical data)\n",
    "\n",
    "def print_size(feature, id):\n",
    "    print(' '.join(['Choosen', str(len(feature)), 'words from document', str(id)]))\n",
    "\n",
    "# The numerical parameters in here can be tuned\n",
    "f1 = feature_extractor(d1, fd1, 7, 7, my_stopwords)\n",
    "f2 = feature_extractor(d2, fd2, 7, 7, my_stopwords)\n",
    "f3 = feature_extractor(d3, fd3, 7, 7, my_stopwords)\n",
    "\n",
    "print_size(f1, 1)\n",
    "print_size(f2, 2)\n",
    "print_size(f3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosen 121 words from document \"union features\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('testing', 251),\n",
       " ('students', 228),\n",
       " ('substance', 164),\n",
       " ('student', 145),\n",
       " ('schools', 116),\n",
       " ('marijuana', 109),\n",
       " ('reported', 62),\n",
       " ('extracurricular', 61),\n",
       " ('alcohol', 57),\n",
       " ('activities', 47),\n",
       " ('illicit', 47),\n",
       " ('national', 40),\n",
       " ('prevalence', 38),\n",
       " ('adolescent', 35),\n",
       " ('subject', 35),\n",
       " ('athlete', 34),\n",
       " ('results', 33),\n",
       " ('predominant', 32),\n",
       " ('education', 32),\n",
       " ('polysubstance', 32),\n",
       " ('treatment', 30),\n",
       " ('classes', 29),\n",
       " ('control', 28),\n",
       " ('population', 27),\n",
       " ('nonathlete', 27),\n",
       " ('institute', 27),\n",
       " ('associated', 26),\n",
       " ('substances', 25),\n",
       " ('significantly', 25),\n",
       " ('covered', 25),\n",
       " ('participation', 24),\n",
       " ('journal', 24),\n",
       " ('ethnicity', 23),\n",
       " ('effects', 23),\n",
       " ('significant', 22),\n",
       " ('measures', 22),\n",
       " ('associations', 21),\n",
       " ('perceived', 21),\n",
       " ('research', 21),\n",
       " ('districts', 20),\n",
       " ('drinking', 20),\n",
       " ('depressive', 20),\n",
       " ('symptoms', 19),\n",
       " ('consequences', 19),\n",
       " ('athletes', 19),\n",
       " ('general', 19),\n",
       " ('somatic', 18),\n",
       " ('evidence', 18),\n",
       " ('covariates', 18),\n",
       " ('american', 18),\n",
       " ('findings', 18),\n",
       " ('negative', 18),\n",
       " ('attitudes', 16),\n",
       " ('program', 16),\n",
       " ('specified', 16),\n",
       " ('current', 15),\n",
       " ('baseline', 15),\n",
       " ('positive', 15),\n",
       " ('frequency', 15),\n",
       " ('medication', 14),\n",
       " ('activity', 14),\n",
       " ('included', 14),\n",
       " ('adolescents', 14),\n",
       " ('nonathletic', 14),\n",
       " ('michigan', 13),\n",
       " ('disorders', 13),\n",
       " ('consent', 13),\n",
       " ('tobacco', 12),\n",
       " ('mcelrath', 12),\n",
       " ('district', 12),\n",
       " ('participants', 11),\n",
       " ('studies', 11),\n",
       " ('psychiatry', 11),\n",
       " ('smoking', 11),\n",
       " ('structure', 11),\n",
       " ('following', 11),\n",
       " ('percentage', 10),\n",
       " ('impacts', 10),\n",
       " ('participating', 10),\n",
       " ('demographic', 10),\n",
       " ('indicate', 10),\n",
       " ('statistics', 10),\n",
       " ('disapproval', 10),\n",
       " ('whether', 9),\n",
       " ('available', 9),\n",
       " ('burdumy', 9),\n",
       " ('differences', 9),\n",
       " ('statistically', 9),\n",
       " ('occasions', 9),\n",
       " ('university', 9),\n",
       " ('707e715', 9),\n",
       " ('participated', 9),\n",
       " ('specific', 8),\n",
       " ('hispanic', 8),\n",
       " ('spillover', 8),\n",
       " ('analysis', 8),\n",
       " ('average', 8),\n",
       " ('cigarette', 8),\n",
       " ('including', 8),\n",
       " ('716e723', 8),\n",
       " ('services', 8),\n",
       " ('compared', 8),\n",
       " ('outcomes', 8),\n",
       " ('information', 7),\n",
       " ('limited', 7),\n",
       " ('similar', 7),\n",
       " ('increased', 7),\n",
       " ('perceive', 7),\n",
       " ('attending', 7),\n",
       " ('moderate', 7),\n",
       " ('connection', 7),\n",
       " ('probabilities', 7),\n",
       " ('disapprove', 7),\n",
       " ('children', 7),\n",
       " ('adolescence', 7),\n",
       " ('related', 7),\n",
       " ('graduate', 7),\n",
       " ('college', 7),\n",
       " ('without', 7),\n",
       " ('cigarettes', 7),\n",
       " ('african', 7)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a dictionary of word-frequency pairs from the union of several feature sets\n",
    "def union_choosen_features(*args):\n",
    "    if (not all([type(arg) is set for arg in args])):\n",
    "        raise ValueError('Input must be sets of word-frequency pairs.')\n",
    "    if (len(args) < 2):\n",
    "        raise ValueError('At least 2 arguments needed.')\n",
    "    \n",
    "    N = len(args)  # number of documents\n",
    "    result = dict()  # word:frequency pairs\n",
    "    for i in range(N):\n",
    "        for w in args[i]:  # access each feature set\n",
    "            if w[0] in result.keys():\n",
    "                result[w[0]] += w[1]\n",
    "            else:\n",
    "                result[w[0]] = w[1]\n",
    "    return result\n",
    "        \n",
    "union_features = union_choosen_features(f1, f2, f3)\n",
    "print_size(union_features, '\\\"union features\\\"')\n",
    "sorted(all_features.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer 0 Integer 1 Integer 2\n"
     ]
    }
   ],
   "source": [
    "# The way to assign a sequence of variables (you can safely ignore this part)\n",
    "for i in range(3):\n",
    "    vars()[''.join(['arg', str(i)])] = ' '.join(['Integer', str(i)])\n",
    "print(arg0, arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}